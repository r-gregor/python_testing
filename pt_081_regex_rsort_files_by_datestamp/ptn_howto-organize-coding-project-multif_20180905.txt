filename: jv_ptn_howto-organize-coding-project-multif_20180905.txt
https://stackoverflow.com/questions/14336576/how-to-organize-java-project-with-ide

How to organize java project with IDE?

***
   A professional solution is not to rely on an IDE for building your software. You want to use a build
   tool to build the software. There are many build tools available to choose from, e.g.
    1. Make
    2. ANT
    3. Maven
    4. Rake
    5. Gradle
    6. Buildr

   etc.

   The tool you choose depends on how you see building software.

   For example tools such as Make, ANT, etc take a “do exactly as I say” approach to building software.
   This can be good if you like having to spell out exactly what you want done and how you want it done.

   Tools such as Maven take a “convention over configuration” approach. They say if you follow the
   conventions, you shouldn't have to repeatedly tell the build tool how to do standard things, so for
   example with Maven if you put your Java source code in the src/main/java directory, then Maven will
   automatically compile it for you, if you put your test source code in src/test/java then Maven will
   compile that for you and run all the tests.

   Maven is not for everyone... some people seem to want to spend more of their time telling the build
   tool exactly what to do and less time writing their software... that's fine... it's not me (I am on
   the Maven PMC... no surprises for guessing what my favourite build tool is)... but these people do
   like some of the “convention over configuration” stuff that Maven gives... they just want a more
   flexible way of overriding when they need to step away from the convention... tools such as Gradle
   and Buildr are used by people with that mind-set.

   Good IDEs will take the build file and infer the project structure from that build tool. IntelliJ,
   Eclipse and NetBeans all understand Maven's pom.xml build file. It makes getting your IDE set up a
   no-op. I have not investigated how the other build tools fair in IDEs as I personally use Maven and
   IntelliJ.

   Pick a build tool and see how that works for you. I assume you are using a Version Control System...
   if you are then changing build tool should not be a big deal.

   If you choose to use Maven, you will start with a 4 or 5 module project (a parent module and three
   child modules with possibly a shared common module)

   The code will be structured like so
+- pom.xml (the root parent pom)
+- common (the common module directory)
|    +- pom.xml (the common module pom)
|    \- src
|         +- main
|         |    +- java
|         |    |    \- com... (your package dirs for common classes)
|         |    \- resources
|         |         \- com... (your package dirs for common classpath resources)
|         \- test
|              +- java
|              |    \- com... (your package dirs for tests of common classes)
|              \- resources
|                   \- com... (your package dirs for common test classpath resources)
+- app1 (the app1 module directory)
|    +- pom.xml (the app1 module pom)
|    \- src
|         +- main
|         |    +- java
|         |    |    \- com...
|         |    \- resources
|         |         \- com...
|         \- test
|              +- java
|              |    \- com...
|              \- resources
|                   \- com...
+- app2 (the app2 module directory)
|    +- pom.xml (the app2 module pom)
|    \- src
|         +- main
|         |    +- java
|         |    |    \- com...
|         |    \- resources
|         |         \- com...
|         \- test
|              +- java
|              |    \- com...
|              \- resources
|                   \- com...
\- app3 (the app3 module directory)
     +- pom.xml (the app3 module pom)
     \- src
          +- main
          |    +- java
          |    |    \- com...
          |    \- resources
          |         \- com...
          \- test
               +- java
               |    \- com...
               \- resources
                    \- com...

   Your root pom.xml will look something like
<project>
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.mydomain.something</groupId>
    <artifactId>something-parent</artifactId>
    <version>1.0-SNAPSHOT</version>
    <packaging>pom</packaging>
    <modules>
        <module>common</module>
        <module>app1</module>
        <module>app2</module>
        <module>app3</module>
    </modules>
</project>

   And the common/pom.xml will look something like
<project>
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>com.mydomain.something</groupId>
        <artifactId>something-parent</artifactId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <artifactId>something-common</artifactId>
    <dependencies>
        <!-- insert the 3rd party dependencies you want -->
    </dependencies>
</project>

   And then each of the app poms will look something like this
<project>
    <modelVersion>4.0.0</modelVersion>
    <parent>
        <groupId>com.mydomain.something</groupId>
        <artifactId>something-parent</artifactId>
        <version>1.0-SNAPSHOT</version>
    </parent>
    <artifactId>something-app1</artifactId>
    <dependencies>
        <dependency>
            <groupId>com.mydomain.something</groupId>
            <artifactId>something-common</artifactId>
            <version>1.0-SNAPSHOT</version>
        </dependency>
        <!-- insert any 3rd party dependencies you want for app1 only -->
    </dependencies>
</project>

***
   I use the following approach: root-project consists of some subprojects. Some of them depends on
   others (this is configured in project properties) to use the code base of project depends on.
root-project
+---- project1
+---- project2
+---- project3
+---- shared-code-project

***
   If truly professionally, assume you also need to build your project with Maven and layout folders
   along the Maven concepts. Even if you will never build from command line, most of recent good
   IDEs should support Maven projects. It is very likely that you may need to use Maven in the future
   and it will be less work to migrate.

   UPDATE: This was at the time of writing. Now I would suggest to use [gradle that is somewhat
   getting more popularity over Maven.


---
https://stackoverflow.com/questions/210567/package-structure-for-a-java-project

Package structure for a Java project?

   Whats the best practice for setting up package structures in a Java Web Application?

***
   You could follow maven's standard project layout. You don't have to actually use maven, but it
   would make the transition easier in the future (if necessary). Plus, other developers will be used to
   seeing that layout, since many open source projects are layed out this way,
--------------------------------------------------------------------------------------------------------------
Introduction to the Standard Directory Layout
[http://maven.apache.org/guides/introduction/introduction-to-the-standard-directory-layout.html]

Having a common directory layout would allow for users familiar with one Maven project to immediately feel at
home in another Maven project. The advantages are analogous to adopting a site-wide look-and-feel.

The next section documents the directory layout expected by Maven and the directory layout created by Maven.
Please try to conform to this structure as much as possible; however, if you can't these settings can be
overridden via the project descriptor.

┌──────────────────┬──────────────────────────────────────────────────────────────────────────┐
│src/main/java     │Application/Library sources                                               │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/main/resources│Application/Library resources                                             │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/main/filters  │Resource filter files                                                     │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/main/webapp   │Web application sources                                                   │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/test/java     │Test sources                                                              │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/test/resources│Test resources                                                            │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/test/filters  │Test resource filter files                                                │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/it            │Integration Tests (primarily for plugins)                                 │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/assembly      │Assembly descriptors                                                      │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│src/site          │Site                                                                      │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│LICENSE.txt       │Project's license                                                         │
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│NOTICE.txt        │Notices and attributions required by libraries that the project depends on│
├──────────────────┼──────────────────────────────────────────────────────────────────────────┤
│README.txt        │Project's readme                                                          │
└──────────────────┴──────────────────────────────────────────────────────────────────────────┘

At the top level, files descriptive of the project: a pom.xml file. In addition, there are textual documents
meant for the user to be able to read immediately on receiving the source: README.txt, LICENSE.txt, etc.

There are just two subdirectories of this structure: src and target. The only other directories that would be
expected here are metadata like CVS, .git or .svn, and any subprojects in a multiproject build (each of which
would be laid out as above).

The target directory is used to house all output of the build.

The src directory contains all of the source material for building the project, its site and so on. It
contains a subdirectory for each type: main for the main build artifact, test for the unit test code and
resources, site and so on.

Within artifact producing source directories (ie. main and test), there is one directory for the language
java (under which the normal package hierarchy exists), and one for resources (the structure which is copied
to the target classpath given the default resource definition).

If there are other contributing sources to the artifact build, they would be under other subdirectories: for
example src/main/antlr would contain Antlr grammar definition files.
--------------------------------------------------------------------------------------------------------------
       
	   I also recommend using Maven's layout if you have a choice. It's a well-thought out structure
       that has been battle-tested, and is familiar to many developers. – [45]Dov Wasserman Oct 17 '08
       at 4:46
     * 13
       You can use this oneliner to create the directory layout: mkdir -p
       src/{main/{java,resources,filters,assembly,config,webapp},test/{java,resources,filters},site}
 

***
   I would suggest creating your package structure by feature, and not by the implementation layer. A
   good write up on this is [58]Java practices: Package by feature, not layer
--------------------------------------------------------------------------------------------------------------
Package by feature, not layer
[http://www.javapractices.com/topic/TopicAction.do;jsessionid=0BF4844350780B6F55476E1137FF4893?Id=205]

The first question in building an application is "How do I divide it up into packages?". For typical business
applications, there seems to be two ways of answering this question.

Package By Feature

Package-by-feature uses packages to reflect the feature set. It tries to place all items related to a single
feature (and only that feature) into a single directory/package. This results in packages with high cohesion
and high modularity, and with minimal coupling between packages. Items that work closely together are placed
next to each other. They aren't spread out all over the application. It's also interesting to note that, in
some cases, deleting a feature can reduce to a single operation - deleting a directory. (Deletion operations
might be thought of as a good test for maximum modularity: an item has maximum modularity only if it can be
deleted in a single operation.)

In package-by-feature, the package names correspond to important, high-level aspects of the problem domain.
For example, a drug prescription application might have these packages:

  • com.app.doctor
  • com.app.drug
  • com.app.patient
  • com.app.presription
  • com.app.report
  • com.app.security
  • com.app.webmaster
  • com.app.util
  • and so on...

Each package usually contains only the items related to that particular feature, and no other feature. For
example, the com.app.doctor package might contain these items:

  • DoctorAction.java - an action or controller object
  • Doctor.java - a Model Object
  • DoctorDAO.java - Data Access Object
  • database items (SQL statements)
  • user interface items (perhaps a JSP, in the case of a web app)

It's important to note that a package can contain not just Java code, but other files as well. Indeed, in
order for package-by-feature to really work as desired, all items related to a given feature - from user
interface, to Java code, to database items - must be placed in a single directory dedicated to that feature
(and only that feature).

In some cases, a feature/package will not be used by any other feature in the application. If that's the
case, it may be removed simply by deleting the directory. If it is indeed used by some other feature, then
its removal will not be as simple as a single delete operation.

That is, the package-by-feature idea does not imply that one package can never use items belonging to other
packages. Rather, package-by-feature aggressively prefers package-private as the default scope, and only
increases the scope of an item to public only when needed.

Package By Layer

The competing package-by-layer style is different. In package-by-layer, the highest level packages reflect
the various application "layers", instead of features, as in:

  • com.app.action
  • com.app.model
  • com.app.dao
  • com.app.util

Here, each feature has its implementation spread out over multiple directories, over what might be loosely
called "implementation categories". Each directory contains items that usually aren't closely related to each
other. This results in packages with low cohesion and low modularity, with high coupling between packages. As
a result, editing a feature involves editing files across different directories. In addition, deleting a
feature can almost never be performed in a single operation.

Recommendation: Use Package By Feature

For typical business applications, the package-by-feature style seems to be the superior of the two:

Higher Modularity
As mentioned above, only package-by-feature has packages with high cohesion, high modularity, and low
coupling between packages.

Easier Code Navigation
Maintenance programmers need to do a lot less searching for items, since all items needed for a given task
are usually in the same directory. Some tools that encourage package-by-layer use package naming conventions
to ease the problem of tedious code navigation. However, package-by-feature transcends the need for such
conventions in the first place, by greatly reducing the need to navigate between directories.

Higher Level of Abstraction
Staying at a high level of abstraction is one of programming's guiding principles of lasting value. It makes
it easier to think about a problem, and emphasizes fundamental services over implementation details. As a
direct benefit of being at a high level of abstraction, the application becomes more self-documenting: the
overall size of the application is communicated by the number of packages, and the basic features are
communicated by the package names. The fundamental flaw with package-by-layer style, on the other hand, is
that it puts implementation details ahead of high level abstractions - which is backwards.

Separates Both Features and Layers
The package-by-feature style still honors the idea of separating layers, but that separation is implemented
using separate classes. The package-by-layer style, on the other hand, implements that separation using both
separate classes and separate packages, which doesn't seem necessary or desirable.

Minimizes Scope
Minimizing scope is another guiding principle of lasting value. Here, package-by-feature allows some classes
to decrease their scope from public to package-private. This is a significant change, and will help to
minimize ripple effects. The package-by-layer style, on the other hand, effectively abandons package-private
scope, and forces you to implement nearly all items as public. This is a fundamental flaw, since it doesn't
allow you to minimize ripple effects by keeping secrets.

Better Growth Style
In the package-by-feature style, the number of classes within each package remains limited to the items
related to a specific feature. If a package becomes too large, it may be refactored in a natural way into two
or more packages. The package-by-layer style, on the other hand, is monolithic. As an application grows in
size, the number of packages remains roughly the same, while the number of classes in each package will
increase without bound.

If you still need further convincing, consider the following.

Directory Structure Is Fundamental To Your Code

"As any designer will tell you, it is the first steps in a design process which count for most. The first few
strokes, which create the form, carry within them the destiny of the rest." - Christopher Alexander

(Christopher Alexander is an architect. Without having worked as programmer, he has influenced many people
who think a lot about programming. His early book A Pattern Language was the original inspiration for the
Design Patterns movement. He has thought long and hard about how to build beautiful things, and these
reflections seem to largely apply to software construction as well.)

In a CBC radio interview, Alexander recounted the following story (paraphrased here): "I was working with one
of my students. He was having a very difficult time building something. He just didn't know how to proceed at
all. So I sat with him, and I said this: Listen, start out by figuring out what the most important thing is.
Get that straight first. Get that straight in your mind. Take your time. Don't be too hasty. Think about it
for a while. When you feel that you have found it, when there is no doubt in your mind that it is indeed the
most important thing, then go ahead and make that most important thing. When you have made that most
important thing, ask yourself if you can make it more beautiful. Cut the bullshit, just get it straight in
your head, if you can make it better or not. When that's done, and you feel you cannot make it any better,
then find the next most important thing."

What are the first strokes in an application, which create its overall form? It is the directory structure.
The directory structure is the very first thing encountered by a programmer when browsing source code.
Everything flows from it. Everything depends on it. It is clearly one of the most important aspects of your
source code.

Consider the different reactions of a programmer when encountering different directory structures. For the
package-by-feature style, the thoughts of the application programmer might be like this:

  • "I see. This lists all the top-level features of the app in one go. Nice."
  • "Let's see. I wonder where this item is located....Oh, here it is. And everything else I am going to need
    is right here too, all in the same spot. Excellent."

For the package-by-layer style, however, the thoughts of the application programmer might be more like this:

  • "These directories tell me nothing. How many features in this app? Beats me. It looks exactly the same as
    all the others. No difference at all. Great. Here we go again..."
  • "Hmm. I wonder where this item is located....I guess its parts are all over the app, spread around in all
    these directories. Do I really have all the items I need? I guess we'll find out later."
  • "I wonder if that naming convention is still being followed. If not, I will have to look it up in that
    other directory."
  • "Wow, would you look at the size of this single directory...sheesh."

Package-By-Layer in Other Domains is Ineffective

By analogy, one can see that the package-by-layer style leads to poor results. For example, imagine a car. At
the highest level, a car's 'implementation' is divided this way (package-by-feature) :

  • safety
  • engine
  • steering
  • fuel system
  • and so on...

Now imagine a car whose 'implementation' under the hood is first divided up according to these lower level
categories (package-by-layer) :

  • electrical
  • mechanical
  • hydraulic

In the case of a transmission problem, for example, you might need to tinker around in these three
compartments. This would mean moving from one part of the car to another completely different one. While in
these various compartments, you could 'see' items having absolutely nothing to do with problem you are trying
to solve. They would simply be in the way, always and everywhere distracting you from the real task at hand.
Wouldn't it make more sense if there was a single place having exactly what you need, and nothing else?

As a second example, consider a large bureacracy divided up into various departments (package-by-feature):

  • front office
  • back office
  • accounting
  • personnel
  • mail room

If a package-by-layer style was used, the primary division would be something like :

  • executives
  • managers
  • employees

Now imagine the bureacracy being divided physically according to these three categories. Each manager is
located, for example, with all the other managers, and not with the employees working for them. Would that be
effective? No, it wouldn't.

So why should software be any different? It seems that package-by-layer is just a bad habit waiting to be
broken.
--------------------------------------------------------------------------------------------------------------

***
   I usually like to have the following:
     * bin (Binaries)
     * doc (Documents)
     * inf (Information)
     * lib (Libraries)
     * res (Resources)
     * src (Source)
     * tst (Test)

***   
   The way i usually have my hierarchy of folder-
     * Project Name
          + src
          + bin
          + tests
          + libs
          + docs


---
https://softwareengineering.stackexchange.com/questions/338597/folder-by-type-or-folder-by-feature

Folder-by-type or Folder-by-feature

   I make use of an AngularJS style guide. Within this guide there is a style called folder-by-feature,
   instead of folder-by-type, and I'm actually curious what's the best approach (in this example for
   Java)

   Let's say I have an application where I can retrieve Users & Pets, using services, controllers,
   repositories and ofcourse domain objects.

   Taking the folder-by-..... styles, we have two options for our packaging structure:

   1. Folder-by-type
com.example
├── domain
│    ├── User.java
│    └── Pet.java
├── controllers
│    ├── UserController.java
│    └── PetController.java
├── repositories
│    ├── UserRepository.java
│    └── PetRepository.java
├── services
│    ├── UserService.java
│    └── PetService.java
│   // and everything else in the project
└── MyApplication.java

   2. Folder-by-feature
com.example
├── pet
│    ├── Pet.java
│    ├── PetController.java
│    ├── PetRepository.java
│    └── PetService.java
├── user
│    ├── User.java
│    ├── UserController.java
│    ├── UserRepository.java
│    └── UserService.java
│   // and everything else in the project
└── MyApplication.java

   What would be a good approach, and what are the arguments to do so?

***   
Folder-by-type only works on small-scale projects. Folder-by-feature is superior in the majority of cases.

   Folder-by-type is ok when you only have a small number of files (under 10 per type lets say). As soon
   as you get multiple components in your project, all with multiple files of the same type, it gets
   very hard to find the actual file you are looking for.

   Therefore, folder-by-feature is better due to its scalability. However if you folder by feature you
   end up losing information about the type of component a file represents (because its no longer in a
   controller folder lets say), so this too becomes confusing. There are 2 simple solutions for this.

   First, you can abide by common naming conventions that imply typeness in the file name. For example
   John Papa's popular AngularJS style guide has the following:

Naming Guidelines
     * Use consistent names for all components following a pattern that describes the component's
       feature then (optionally) its type. My
       recommended pattern is feature.type.js. There are 2 names for most
       assets:
          + the file name (avengers.controller.js)
          + the registered component name with Angular (AvengersController)

   Second, you can combine folder-by-type and folder-by-feature styles into folder-by-feature-by-type:
com.example
├── pet
├────Controllers
│    ├── PetController1.java
|    └── PetController2.java
├────Services
│    ├── PetService1.java
│    └── PetService2.java
├── user
├────Controllers
│    ├── UserController1.java
│    ├── UserController2.java
├────Services
│    ├── UserService1.java
│    └── UserService2.java

***
   This really has nothing to do with the technology in question, unless you use a framework that forces
   folder-by-type on you as part of a convention-over-configuration approach.

   Personally, I am strongly of the opinion, that folder-by-feature is far superior and should be used
   everywhere as much as possible. It groups together classes that actually work together, whereas
   folder-by-type just duplicates something that is usually already present in the class name.

***   
   I have found it to be easier to work with packages-by-feature for one reason mainly:

   High modularity and cohesion: It makes easier to play with components' scope. We can set protected or
   package classes/methods which can only be accessed by components directly related with the feature
   (Law of Demeter).

   Other reasons are:
     * Easier code navigation
     * Higher level of abstraction
     * Minimise scopes (bounding contexts)
     * Easier to scale, maintain and replace.


---
http://insights.sigasi.com/tech/how-organize-source-code-learn-java.html

How to organize source code? Learn from Java.

   Since most HDLs do not force you to use a specific directory structure in your projects, each
   organization is left with the burden to [95]devise its own conventions. This post describes directory
   and file naming conventions, learned from the [96]Java programming language.

Basic structure
   The basic design unit in Java is the Java Class. Each Java class has its own file, with the file name
   identical to the class name, and the extentions .java. Java classes are further organized in Java
   packages, which are visible in the source code. Java packages correspond directly to the directory
   structure of a Java project. The convention for package naming is that package names correspond to
   the domain name of the organization that provides the package.

   An trivial example will make things clear. I've created a trivial hello-world example in the folder
   c:\projectdir\. The example consists of a single class HelloWorld (naming convention in Java is that
   class names start with a capital, and use [97]CamelCase) in the package com.sigasi.example.

   The only file we need for this project is:
c:\projectdir\com\sigasi\example\HelloWorld.java</pre>

   When several subprojects are aggregated, each project has to conform to the directory structure:
c:\projectdir1\com\sigasi\reusable\HelloUtils.java
c:\projectdir2\com\sigasi\example\HelloWorld.java

   The only thing a Java tool needs to know to process this aggregated project, is the root directories
   of each subproject. In this case c:\projectdir1 and c:\projectdir1. This way, you do not have to list
   which files should be included or excluded in your build. The convention is simple: all files will be
   compiled.

   Note that package names start with internet domain names, in inverse order (here: sigasi.com). This
   avoids name clashes, since each organization can create its own package structure within its own
   domain.

Improvement: source and binary dirs
   When compiling the previous examples with a standard Java tool, the binaries (class files, in
   Java-speak) will be in the same directory as the source files:
   c:\projectdir1\com\sigasi\reusable\HelloUtils.java
c:\projectdir1\com\sigasi\reusable\HelloUtils.java
c:\projectdir1\com\sigasi\reusable\HelloUtils.class
c:\projectdir2\com\sigasi\example\HelloWorld.java
c:\projectdir2\com\sigasi\example\HelloWorld.class

   This is not desirable. First, when you share the source files (e.g. in a version control system, or a
   tarball) you do not want the binaries. Conversely when shipping the binaries to a customer, you
   probably do not want them to see your source code.

   Most Java projects nowadays split the source and binary files in a separate directory, typically src
   and bin:
c:\projectdir1\src\com\sigasi\reusable\HelloUtils.java
c:\projectdir1\bin\com\sigasi\reusable\HelloUtils.class
c:\projectdir2\src\com\sigasi\example\HelloWorld.java
c:\projectdir2\bin\com\sigasi\example\HelloWorld.class

   As an extra advantage, it is now trivial to remove all compiled files without special scripts or
   pattern matching: just remove the bin directory.

Advantages of Java-style directory structure
   Java-style directory structure has the following advantages:
     * Each project or sub-project has a root directory for its source code.
     * No name clashes: each organization controls its own name space.
     * No need to list which files you want to compile: tools only need to know those root directories
       to do anything
     * If source an binary files are split, it is very easy to share binaries or source files and to
       clean the build.
     * The project root directory is extremely clean. It contains only a few directories (src, bin and
       perhaps doc) and a few build scripts or project descriptors (e.g. build.xml for [98]Ant


---
https://airbrake.io/blog/python/python-best-practices

Python Best Practices: 5 Tips For Better Code

   Like most programming languages, Python offers an extremely powerful development platform to create
   some of the most useful and robust applications imaginable. However, as Spider-Man and Winston
   Churchill have taught us, with great power comes great responsibility. Here are our thoughts on
   Python best practices to help you harness everything Python has to offer.

   Today we’ll examine a few of the critical Python best practices used to create more professional,
   cleaner code.

Properly Structure Your Repository
   Most developers, no matter the language, will begin a new project by settings up a code repository
   and some form of version control.

   While endless debates can (and do) take place arguing about which version control system is best or
   where to host your project repositories, no matter what you decide to use, it’s critical to focus on
   structuring your project and subsequent repository in a suitable fashion.

   For Python, in particular, there are a few key components that should exist within your repository,
   and you should take the time to generate each of these, at least in a basic skeletal form, before
   actual coding begins.
     * License – [root]: This file should be the first thing you add to your project. If you’re
       uncertain, sites like [18]choosealicense.com can help you decide.
     * README – [root]: Whether you choose Markdown, reStructuredText, or even just plain text as your
       format, getting a basic README file in place can help you describe your project, define its
       purpose, and outline the basic functionality.
     * Module Code – [root] or [/module]: Of course this is all for naught if you don’t have any new or
       worthwhile code to create. That said, be sure to place your actual code inside a properly-named
       sub-directory (e.g. /module), or for a single-file project, within root itself.
     * setup.py – [root]: A basic setup.py script is very common in Python projects,
       allowing Distutils to properly build and distribute the modules your project needs. See
       the [19]official documentation for more information on setup.py.
     * requirements.txt – [root]: While not all projects utilize a requirements.txt file, it can be used
       to specify development modules and other dependencies that are required to work on the project
       properly. See the [20]official PIP page for details.
     * Documentation – [/docs]: Documentation is a key component to a well-structured project, and
       typically it is placed in the /docs directory.
     * Tests – [/tests]: Just like documentation, tests also typically exist in most projects, and
       should be placed in their own respective directory.

   All told, this means a basic Python repository structure might look something like this:
		docs/conf.py
		docs/index.rst
		module/__init__.py
		module/core.py
		tests/core.py
		LICENSE
		README.rst
		requirements.txt
		setup.py

Follow Common Style Guidelines
   Python has a system of community-generated proposals known as [21]Python Enhancement Proposals (PEPs)
   which attempt to provide a basic set of guidelines and standards for a wide variety of topics when it
   comes to proper Python development.

   Perhaps one of the most widely known and referenced PEPs ever created is [22]PEP8, which is the
   “Python community Bible” for properly styling your code.

   While there are far too many specific styling guidelines to give examples here, most Python
   developers make a concerted effort to follow the guidelines set forth in PEP8, so all code across the
   community has a similar look and feel.

Immediately Repair Your Broken Windows
   Perhaps you’ve heard of the broken windows theory, which is a criminological theory that states
   that the proper maintenance and monitoring of urban environments in cities, in order to prevent
   lesser crimes, creates a more positive atmosphere that thereby prevents greater crimes from
   occurring.

   While this theory, as it relates to crime prevention, is highly criticized and contested, it has some
   merit when it comes to the world of coding and application development.

   Specifically, when creating a Python application (or any language for that matter), it is almost
   always more beneficial in the long-term to quickly acknowledge and repair broken windows (i.e. broken
   code) immediately. Putting it off to finish a new component or module can and will often lead to
   further, more complex problems down the line.

   It has [24]even been stated that Microsoft, after a terrible production cycle with the original
   version of Microsoft Word, adopted a “zero defects methodology” to their great benefit, always
   focusing on fixing bugs and defects before new production code. While this accounting may be
   apocryphal, there is a certain logic to applying this model during development.

Create Consistent Documentation
   As much as it can feel like a burden at times, proper documentation throughout the lifecycle of a
   project is a cornerstone to clean code. Thankfully, the Python community has made this process fairly
   painless, and it involves the use of three simple tools and
   concepts: reStructredText, Docstrings, and Sphinx.

   reStructredText is a simple plain text markup syntax. It is commonly used for in-line
   documentation, which in the case of Python, allows for documentation to be generated on-the-fly.
   The Quickstart Guide provides a few simple examples of reStructuredText in action.

   While using reStructuredText is the first step to proper documentation, it’s critical to understand
   how to properly generate Docstrings. A Docstring is simply a documentation string that appears prior
   to every module, class, or method within your Python code.

   Once every component of your code contains a properly
   formatted Docstring using reStructuredText markdown, you can move onto using Sphinx, which is the
   go-to tool to generate Python documentation from existing reStructuredText.Sphinx allows
   documentation to easily be exported into a variety of formats, including beautiful HTML, for
   near-automatic online documentation pages. With virtually no effort, your project documentation can
   even be added to the prominent [30]ReadTheDocs documentation repository after every code commit.

Get Acquainted With PyPI
   While using proper syntax and documentation methods will always produce solid, clean code, perhaps
   one of the best tools to improving your use of Python is the epic module repository known
   as PyPI: The Python Package Index.. As a collective repository for thousands of Python projects,
   including nearly every open or public Python project created, PyPI is an undeniably useful resource,
   no matter your Python experience level or project scope.

   There are two primary uses for PyPI: Using existing modules, or adding your own project to PyPI.

   Most projects will initially begin by utilizing existing projects on PyPI, and with nearly 100,000 at
   the time of writing to choose from, there’s almost certainly some code that fits your project’s
   needs. Once you’ve located a module you’re interested in using within your own project, the fun can
   begin.

   Next, you’ll want to utilize the Python Packaging Authority [32]recommendations for installing a
   package from PyPI. Typically this entails using [33]pip, the go-to package installation software. If
   you’re using Python 2 >=2.7.9 or Python 3 >=3.4, pipcomes pre-installed with Python.

   Once pip is installed, simply run the appropriate command, as suggested by the module documentation
   within PyPI. For example, to install [34]requests, a common module to handle HTTP requests, you’d
   simply use the following command from the root directory of your project:
		pip install requests

   pip will then take care of the rest and install the package. Now you can keep adding more modules to
   help your code shine!

   Down the road, if you’ve got a decent module of your own created and wish to share it with the rest
   of the Python community, you’ll want to take a look at the [35]Python Distribution documentation,
   which handles the finer details of packaging your project, as discussed earlier.

   Most importantly, look over the simple steps to [36]Upload Your Project to PyPI, which requires you
   to: Signup for a PyPIaccount, register your project, and then upload it to PyPI for others to view
   and add to their own projects!


---
https://docs.python-guide.org/writing/structure/

Structuring Your Project

   By “structure” we mean the decisions you make concerning how your project best meets its objective.
   We need to consider how to best leverage Python’s features to create clean, effective code. In
   practical terms, “structure” means making clean code whose logic and dependencies are clear as well
   as how the files and folders are organized in the filesystem.

   Which functions should go into which modules? How does data flow through the project? What features
   and functions can be grouped together and isolated? By answering questions like these you can begin
   to plan, in a broad sense, what your finished product will look like.

   In this section we take a closer look at Python’s module and import systems as they are the central
   elements to enforcing structure in your project. We then discuss various perspectives on how to build
   code which can be extended and tested reliably.

Structure of the Repository

It’s Important.

   Just as Code Style, API Design, and Automation are essential for a healthy development cycle,
   Repository structure is a crucial part of your project’s architecture.

   When a potential user or contributor lands on your repository’s page, they see a few things:
     * Project Name
     * Project Description
     * Bunch O’ Files

   Only when they scroll below the fold will the user see your project’s README.

   If your repo is a massive dump of files or a nested mess of directories, they might look elsewhere
   before even reading your beautiful documentation.

   Dress for the job you want, not the job you have.

   Of course, first impressions aren’t everything. You and your colleagues will spend countless hours
   working with this repository, eventually becoming intimately familiar with every nook and cranny. The
   layout of it is important.

Sample Repository

   This repository is available on GitHub.
README.rst
LICENSE
setup.py
requirements.txt
sample/__init__.py
sample/core.py
sample/helpers.py
docs/conf.py
docs/index.rst
tests/test_basic.py
tests/test_advanced.py

   Let’s get into some specifics.

The Actual Module
   Location ./sample/ or ./sample.py
   Purpose  The code of interest

   Your module package is the core focus of the repository. It should not be tucked away:
./sample/

   If your module consists of only a single file, you can place it directly in the root of your
   repository:
./sample.py

   Your library does not belong in an ambiguous src or python subdirectory.

License
   Location ./LICENSE
   Purpose  Lawyering up.

   This is arguably the most important part of your repository, aside from the source code itself. The
   full license text and copyright claims should exist in this file.

   If you aren’t sure which license you should use for your project, check out [14]choosealicense.com.

   Of course, you are also free to publish code without a license, but this would prevent many people
   from potentially using your code.

Setup.py
   Location ./setup.py
   Purpose  Package and distribution management.

   If your module package is at the root of your repository, this should obviously be at the root as
   well.

Requirements File
   Location ./requirements.txt
   Purpose  Development dependencies.

   A [17]pip requirements file should be placed at the root of the repository. It should specify the
   dependencies required to contribute to the project: testing, building, and generating documentation.

   If your project has no development dependencies, or you prefer development environment setup via
   setup.py, this file may be unnecessary.

Documentation
   Location ./docs/
   Purpose  Package reference documentation.

   There is little reason for this to exist elsewhere.

Test Suite
   For advice on writing your tests, see [20]Testing Your Code.
   Location ./test_sample.py or ./tests
   Purpose  Package integration and unit tests.

   Starting out, a small test suite will often exist in a single file:
./test_sample.py

   Once a test suite grows, you can move your tests to a directory, like so:
tests/test_basic.py
tests/test_advanced.py

   Obviously, these test modules must import your packaged module to test it. You can do this a few
   ways:
     * Expect the package to be installed in site-packages.
     * Use a simple (but explicit) path modification to resolve the package properly.

   I highly recommend the latter. Requiring a developer to run setup.py develop to test an actively
   changing codebase also requires them to have an isolated environment setup for each instance of the
   codebase.

   To give the individual tests import context, create a tests/context.py file:
import os
import sys
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import sample

   Then, within the individual test modules, import the module like so:
from .context import sample

   This will always work as expected, regardless of installation method.

   Some people will assert that you should distribute your tests within your module itself – I disagree.
   It often increases complexity for your users; many test suites often require additional dependencies
   and runtime contexts.

Makefile
   Location ./Makefile
   Purpose  Generic management tasks.

   If you look at most of my projects or any Pocoo project, you’ll notice a Makefile laying around. Why?
   These projects aren’t written in C… In short, make is a incredibly useful tool for defining generic
   tasks for your project.

   Sample Makefile:
init:
    pip install -r requirements.txt

test:
    py.test tests

.PHONY: init test

   Other generic management scripts (e.g. manage.py or fabfile.py) belong at the root of the repository
   as well.

Regarding Django Applications
   I’ve noticed a new trend in Django applications since the release of Django 1.4. Many developers are
   structuring their repositories poorly due to the new bundled application templates.

   How? Well, they go to their bare and fresh repository and run the following, as they always have:
$ django-admin.py startproject samplesite

   The resulting repository structure looks like this:
README.rst
samplesite/manage.py
samplesite/samplesite/settings.py
samplesite/samplesite/wsgi.py
samplesite/samplesite/sampleapp/models.py

   Don’t do this.

   Repetitive paths are confusing for both your tools and your developers. Unnecessary nesting doesn’t
   help anybody (unless they’re nostalgic for monolithic SVN repos).

   Let’s do it properly:
$ django-admin.py startproject samplesite .

   Note the “.”.

   The resulting structure:
README.rst
manage.py
samplesite/settings.py
samplesite/wsgi.py
samplesite/sampleapp/models.py

Structure of Code is Key
   Thanks to the way imports and modules are handled in Python, it is relatively easy to structure a
   Python project. Easy, here, means that you do not have many constraints and that the module importing
   model is easy to grasp. Therefore, you are left with the pure architectural task of crafting the
   different parts of your project and their interactions.

   Easy structuring of a project means it is also easy to do it poorly. Some signs of a poorly
   structured project include:
     * Multiple and messy circular dependencies: if your classes Table and Chair in furn.py need to
       import Carpenter from workers.py to answer a question such as table.isdoneby(), and if conversely
       the class Carpenter needs to import Table and Chair, to answer the question carpenter.whatdo(),
       then you have a circular dependency. In this case you will have to resort to fragile hacks such
       as using import statements inside methods or functions.
     * Hidden coupling: each and every change in Table’s implementation breaks 20 tests in unrelated
       test cases because it breaks Carpenter’s code, which requires very careful surgery to adapt the
       change. This means you have too many assumptions about Table in Carpenter’s code or the reverse.
     * Heavy usage of global state or context: instead of explicitly passing (height, width, type, wood)
       to each other, Table and Carpenter rely on global variables that can be modified and are modified
       on the fly by different agents. You need to scrutinize all access to these global variables to
       understand why a rectangular table became a square, and discover that remote template code is
       also modifying this context, messing with table dimensions.
     * Spaghetti code: multiple pages of nested if clauses and for loops with a lot of copy-pasted
       procedural code and no proper segmentation are known as spaghetti code. Python’s meaningful
       indentation (one of its most controversial features) make it very hard to maintain this kind of
       code. So the good news is that you might not see too much of it.
     * Ravioli code is more likely in Python: it consists of hundreds of similar little pieces of logic,
       often classes or objects, without proper structure. If you never can remember if you have to use
       FurnitureTable, AssetTable or Table, or even TableNew for your task at hand, you might be
       swimming in ravioli code.

Modules
   Python modules are one of the main abstraction layers available and probably the most natural one.
   Abstraction layers allow separating code into parts holding related data and functionality.

   For example, a layer of a project can handle interfacing with user actions, while another would
   handle low-level manipulation of data. The most natural way to separate these two layers is to
   regroup all interfacing functionality in one file, and all low-level operations in another file. In
   this case, the interface file needs to import the low-level file. This is done with the import and
   from ... import statements.

   As soon as you use import statements you use modules. These can be either built-in modules such as os
   and sys, third-party modules you have installed in your environment, or your project’s internal
   modules.

   To keep in line with the style guide, keep module names short, lowercase, and be sure to avoid using
   special symbols like the dot (.) or question mark (?). So a file name like my.spam.py is one you
   should avoid! Naming this way will interfere with the way Python looks for modules.

   In the case of my.spam.py Python expects to find a spam.py file in a folder named my which is not the
   case. There is an [25]example of how the dot notation should be used in the Python docs.

   If you’d like you could name your module my_spam.py, but even our friend the underscore should not be
   seen often in module names. However, using other characters (spaces or hyphens) in module names will
   prevent importing (- is the subtract operator), so try to keep module names short so there is no need
   to separate words. And, most of all, don’t namespace with underscores, use submodules instead.
# OK
import library.plugin.foo
# not OK
import library.foo_plugin

   Aside from some naming restrictions, nothing special is required for a Python file to be a module,
   but you need to understand the import mechanism in order to use this concept properly and avoid some
   issues.

   Concretely, the import modu statement will look for the proper file, which is modu.py in the same
   directory as the caller if it exists. If it is not found, the Python interpreter will search for
   modu.py in the “path” recursively and raise an ImportError exception if it is not found.

   Once modu.py is found, the Python interpreter will execute the module in an isolated scope. Any
   top-level statement in modu.py will be executed, including other imports if any. Function and class
   definitions are stored in the module’s dictionary.

   Then, the module’s variables, functions, and classes will be available to the caller through the
   module’s namespace, a central concept in programming that is particularly helpful and powerful in
   Python.

   In many languages, an include file directive is used by the preprocessor to take all code found in
   the file and ‘copy’ it into the caller’s code. It is different in Python: the included code is
   isolated in a module namespace, which means that you generally don’t have to worry that the included
   code could have unwanted effects, e.g. override an existing function with the same name.

   It is possible to simulate the more standard behavior by using a special syntax of the import
   statement: from modu import *. This is generally considered bad practice. Using import * makes code
   harder to read and makes dependencies less compartmentalized.

   Using from modu import func is a way to pinpoint the function you want to import and put it in the
   global namespace. While much less harmful than import * because it shows explicitly what is imported
   in the global namespace, its only advantage over a simpler import modu is that it will save a little
   typing.

   Very bad
[...]
from modu import *
[...]
x = sqrt(4)  # Is sqrt part of modu? A builtin? Defined above?

   Better
from modu import sqrt
[...]
x = sqrt(4)  # sqrt may be part of modu, if not redefined in between

   Best
import modu
[...]
x = modu.sqrt(4)  # sqrt is visibly part of modu's namespace

   As mentioned in the [26]Code Style section, readability is one of the main features of Python.
   Readability means to avoid useless boilerplate text and clutter, therefore some efforts are spent
   trying to achieve a certain level of brevity. But terseness and obscurity are the limits where
   brevity should stop. Being able to tell immediately where a class or function comes from, as in the
   modu.func idiom, greatly improves code readability and understandability in all but the simplest
   single file projects.

Packages
   Python provides a very straightforward packaging system, which is simply an extension of the module
   mechanism to a directory.

   Any directory with an __init__.py file is considered a Python package. The different modules in the
   package are imported in a similar manner as plain modules, but with a special behavior for the
   __init__.py file, which is used to gather all package-wide definitions.

   A file modu.py in the directory pack/ is imported with the statement import pack.modu. This statement
   will look for an __init__.py file in pack, execute all of its top-level statements. Then it will look
   for a file named pack/modu.py and execute all of its top-level statements. After these operations,
   any variable, function, or class defined in modu.py is available in the pack.modu namespace.

   A commonly seen issue is to add too much code to __init__.py files. When the project complexity
   grows, there may be sub-packages and sub-sub-packages in a deep directory structure. In this case,
   importing a single item from a sub-sub-package will require executing all __init__.py files met while
   traversing the tree.

   Leaving an __init__.py file empty is considered normal and even a good practice, if the package’s
   modules and sub-packages do not need to share any code.

   Lastly, a convenient syntax is available for importing deeply nested packages: import
   very.deep.module as mod. This allows you to use mod in place of the verbose repetition of
   very.deep.module.

Object-oriented programming
   Python is sometimes described as an object-oriented programming language. This can be somewhat
   misleading and needs to be clarified.

   In Python, everything is an object, and can be handled as such. This is what is meant when we say,
   for example, that functions are first-class objects. Functions, classes, strings, and even types are
   objects in Python: like any object, they have a type, they can be passed as function arguments, and
   they may have methods and properties. In this understanding, Python is an object-oriented language.

   However, unlike Java, Python does not impose object-oriented programming as the main programming
   paradigm. It is perfectly viable for a Python project to not be object-oriented, i.e. to use no or
   very few class definitions, class inheritance, or any other mechanisms that are specific to
   object-oriented programming.

   Moreover, as seen in the [29]modules section, the way Python handles modules and namespaces gives the
   developer a natural way to ensure the encapsulation and separation of abstraction layers, both being
   the most common reasons to use object-orientation. Therefore, Python programmers have more latitude
   to not use object-orientation, when it is not required by the business model.

   There are some reasons to avoid unnecessary object-orientation. Defining custom classes is useful
   when we want to glue together some state and some functionality. The problem, as pointed out by the
   discussions about functional programming, comes from the “state” part of the equation.

   In some architectures, typically web applications, multiple instances of Python processes are spawned
   to respond to external requests that can happen at the same time. In this case, holding some state
   into instantiated objects, which means keeping some static information about the world, is prone to
   concurrency problems or race-conditions. Sometimes, between the initialization of the state of an
   object (usually done with the __init__() method) and the actual use of the object state through one
   of its methods, the world may have changed, and the retained state may be outdated. For example, a
   request may load an item in memory and mark it as read by a user. If another request requires the
   deletion of this item at the same time, it may happen that the deletion actually occurs after the
   first process loaded the item, and then we have to mark as read a deleted object.

   This and other issues led to the idea that using stateless functions is a better programming
   paradigm.

   Another way to say the same thing is to suggest using functions and procedures with as few implicit
   contexts and side-effects as possible. A function’s implicit context is made up of any of the global
   variables or items in the persistence layer that are accessed from within the function. Side-effects
   are the changes that a function makes to its implicit context. If a function saves or deletes data in
   a global variable or in the persistence layer, it is said to have a side-effect.

   Carefully isolating functions with context and side-effects from functions with logic (called pure
   functions) allow the following benefits:
     * Pure functions are deterministic: given a fixed input, the output will always be the same.
     * Pure functions are much easier to change or replace if they need to be refactored or optimized.
     * Pure functions are easier to test with unit-tests: There is less need for complex context setup
       and data cleaning afterwards.
     * Pure functions are easier to manipulate, decorate, and pass around.

   In summary, pure functions are more efficient building blocks than classes and objects for some
   architectures because they have no context or side-effects.

   Obviously, object-orientation is useful and even necessary in many cases, for example when developing
   graphical desktop applications or games, where the things that are manipulated (windows, buttons,
   avatars, vehicles) have a relatively long life of their own in the computer’s memory.

Decorators
   The Python language provides a simple yet powerful syntax called ‘decorators’. A decorator is a
   function or a class that wraps (or decorates) a function or a method. The ‘decorated’ function or
   method will replace the original ‘undecorated’ function or method. Because functions are first-class
   objects in Python, this can be done ‘manually’, but using the @decorator syntax is clearer and thus
   preferred.
def foo():
    # do something

def decorator(func):
    # manipulate func
    return func

foo = decorator(foo)  # Manually decorate

@decorator
def bar():
    # Do something
# bar() is decorated

   This mechanism is useful for separating concerns and avoiding external un-related logic ‘polluting’
   the core logic of the function or method. A good example of a piece of functionality that is better
   handled with decoration is [31]memoization or caching: you want to store the results of an expensive
   function in a table and use them directly instead of recomputing them when they have already been
   computed. This is clearly not part of the function logic.

Context Managers
   A context manager is a Python object that provides extra contextual information to an action. This
   extra information takes the form of running a callable upon initiating the context using the with
   statement, as well as running a callable upon completing all the code inside the with block. The most
   well known example of using a context manager is shown here, opening on a file:
with open('file.txt') as f:
    contents = f.read()

   Anyone familiar with this pattern knows that invoking open in this fashion ensures that f’s close
   method will be called at some point. This reduces a developer’s cognitive load and makes the code
   easier to read.

   There are two easy ways to implement this functionality yourself: using a class or using a generator.
   Let’s implement the above functionality ourselves, starting with the class approach:
class CustomOpen(object):
    def __init__(self, filename):
        self.file = open(filename)

    def __enter__(self):
        return self.file

    def __exit__(self, ctx_type, ctx_value, ctx_traceback):
        self.file.close()

with CustomOpen('file') as f:
    contents = f.read()

   This is just a regular Python object with two extra methods that are used by the with statement.
   CustomOpen is first instantiated and then its __enter__ method is called and whatever __enter__
   returns is assigned to f in the as f part of the statement. When the contents of the with block is
   finished executing, the __exit__ method is then called.

   And now the generator approach using Python’s own [33]contextlib:
from contextlib import contextmanager

@contextmanager
def custom_open(filename):
    f = open(filename)
    try:
        yield f
    finally:
        f.close()

with custom_open('file') as f:
    contents = f.read()

   This works in exactly the same way as the class example above, albeit it’s more terse. The
   custom_open function executes until it reaches the yield statement. It then gives control back to the
   with statement, which assigns whatever was yield’ed to f in the as f portion. The finally clause
   ensures that close() is called whether or not there was an exception inside the with.

   Since the two approaches appear the same, we should follow the Zen of Python to decide when to use
   which. The class approach might be better if there’s a considerable amount of logic to encapsulate.
   The function approach might be better for situations where we’re dealing with a simple action.

Dynamic typing
   Python is dynamically typed, which means that variables do not have a fixed type. In fact, in Python,
   variables are very different from what they are in many other languages, specifically
   statically-typed languages. Variables are not a segment of the computer’s memory where some value is
   written, they are ‘tags’ or ‘names’ pointing to objects. It is therefore possible for the variable
   ‘a’ to be set to the value 1, then to the value ‘a string’, then to a function.

   The dynamic typing of Python is often considered to be a weakness, and indeed it can lead to
   complexities and hard-to-debug code. Something named ‘a’ can be set to many different things, and the
   developer or the maintainer needs to track this name in the code to make sure it has not been set to
   a completely unrelated object.

   Some guidelines help to avoid this issue:
     * Avoid using the same variable name for different things.

   Bad
a = 1
a = 'a string'
def a():
    pass  # Do something

   Good
count = 1
msg = 'a string'
def func():
    pass  # Do something

   Using short functions or methods helps reduce the risk of using the same name for two unrelated
   things.

   It is better to use different names even for things that are related, when they have a different
   type:

   Bad
items = 'a b c d'  # This is a string...
items = items.split(' ')  # ...becoming a list
items = set(items)  # ...and then a set

   There is no efficiency gain when reusing names: the assignments will have to create new objects
   anyway. However, when the complexity grows and each assignment is separated by other lines of code,
   including ‘if’ branches and loops, it becomes harder to ascertain what a given variable’s type is.

   Some coding practices, like functional programming, recommend never reassigning a variable. In Java
   this is done with the final keyword. Python does not have a final keyword and it would be against its
   philosophy anyway. However, it may be a good discipline to avoid assigning to a variable more than
   once, and it helps in grasping the concept of mutable and immutable types.

Mutable and immutable types
   Python has two kinds of built-in or user-defined types.

   Mutable types are those that allow in-place modification of the content. Typical mutables are lists
   and dictionaries: All lists have mutating methods, like list.append() or list.pop(), and can be
   modified in place. The same goes for dictionaries.

   Immutable types provide no method for changing their content. For instance, the variable x set to the
   integer 6 has no “increment” method. If you want to compute x + 1, you have to create another integer
   and give it a name.
my_list = [1, 2, 3]
my_list[0] = 4
print my_list  # [4, 2, 3] <- The same list has changed

x = 6
x = x + 1  # The new x is another object

   One consequence of this difference in behavior is that mutable types are not “stable”, and therefore
   cannot be used as dictionary keys.

   Using properly mutable types for things that are mutable in nature and immutable types for things
   that are fixed in nature helps to clarify the intent of the code.

   For example, the immutable equivalent of a list is the tuple, created with (1, 2). This tuple is a
   pair that cannot be changed in-place, and can be used as a key for a dictionary.

   One peculiarity of Python that can surprise beginners is that strings are immutable. This means that
   when constructing a string from its parts, it is much more efficient to accumulate the parts in a
   list, which is mutable, and then glue (‘join’) the parts together when the full string is needed. One
   thing to notice, however, is that list comprehensions are better and faster than constructing a list
   in a loop with calls to append().

   One other option is using the map function, which can ‘map’ a function (‘str’) to an iterable
   (‘range(20)’). This results in a map object, which you can then (‘join’) together just like the other
   examples. The map function can be even faster than a list comprehension in some cases.

   Bad
# create a concatenated string from 0 to 19 (e.g. "012..1819")
nums = ""
for n in range(20):
    nums += str(n)   # slow and inefficient
print nums

   Good
# create a concatenated string from 0 to 19 (e.g. "012..1819")
nums = []
for n in range(20):
    nums.append(str(n))
print "".join(nums)  # much more efficient

   Better
# create a concatenated string from 0 to 19 (e.g. "012..1819")
nums = [str(n) for n in range(20)]
print "".join(nums)

   Best
# create a concatenated string from 0 to 19 (e.g. "012..1819")
nums = map(str, range(20))
print "".join(nums)

   One final thing to mention about strings is that using join() is not always best. In the instances
   where you are creating a new string from a pre-determined number of strings, using the addition
   operator is actually faster, but in cases like above or in cases where you are adding to an existing
   string, using join() should be your preferred method.
foo = 'foo'
bar = 'bar'

foobar = foo + bar  # This is good
foo += 'ooo'  # This is bad, instead you should do:
foo = ''.join([foo, 'ooo'])

   Note
   You can also use the % formatting operator to concatenate a pre-determined number of strings
   besides str.join() and +. However, PEP 3101, discourages the usage of the % operator in favor
   of the str.format() method.
foo = 'foo'
bar = 'bar'

foobar = '%s%s' % (foo, bar) # It is OK
foobar = '{0}{1}'.format(foo, bar) # It is better
foobar = '{foo}{bar}'.format(foo=foo, bar=bar) # It is best

Vendorizing Dependencies
Runners


---
https://www.reddit.com/r/Python/comments/22326i/what_is_the_standard_way_to_organize_a_python/

What is the "standard" way to organize a python project ?

   I come from the Java world. In this world, projects are organized around great tools like maven or
   gradle: convention over configuration, standard way to organize directories, nice way to integrate
   plugins and/or new steps.

   As today, with the target of python3, I'm a bit lost in the python world about what is the "typical
   patterns". Examples of packaging are all different and docs seem incomplete ([7]1, [8]2), no default
   to integrate unit tests (many unit test frameworks, not one unify way to call them properly), moving
   target of packages tools (easy_install, pip), no standard build tools ([9]buildout uses eggs, which
   seems to use eggs which are not recommended anymore).

   I understand that Python is about choice, but still I want to know what is (in our opinion) the way
   you're organizing your projects and with what tools.

***   
docs/
moduleName/
setup.py
requirements.txt

   Things are pretty much fair game. I put tests where they logically seem to go, so I don't have one
   big test directory. I use say 5 test folders, one for each major independent group.

     moving target of packages tools (easy_install, pip), no standard build tools (buildout uses eggs,
     which seems to use eggs which are not recommended anymore).

   Eggs are fine. Wheels are the new thing, but they're a long way from being adopted and only matter if
   you bundle C code. Either way, it's shouldn't be a major change.

   If you're writing an open source project, support both pip and easy_install. If you're not, pick one.
   They're very similar.

     no default to integrate unit tests

   unittest is the default. It's pretty good.

***
   If you're going to support a command line interface, use docopt. It's really is amazing.
   [32]http://docopt.org/

***   
   Wow, docopt looks amazing. I hate using argparse and argparse-like tools all the time. Thanks for
   sharing this!

***
https://packaging.python.org/

***
   After reading your comment, I started taking a look at docopt, and it certainly looks cool. The
   problem I'm having is seeing how the options get evaluated.

   All of the examples I was seeing was strictly about the CLI itself, not how the options are then
   interpreted to carry out the task. I dug further and other examples showed how to print out what your
   arguments are, but that's not very useful.

***
   I guess what I'm struggling to grasp is how the parameters are interpreted to do what you want them
   to.

   Like if I have a script that will print out a word, and parameter on my CLI say -n. Which -n expects
   a numerical value. How would that value then get used in my script so that it changes how many times
   that word gets printed out?

***
   I'm missing something.
    1. Create the POSIX blurb.
    2. Pull data from the dictionary.
    3. Use it.
from docopt import docopt
msg = "Usage:\n"
msg += "my_script [-n COUNT]\n"
msg += "my_script -h | --help\n"
msg += "my_script -v | --version\n\n"
msg += "Options:\n"
msg += "  -n COUNT, --count COUNT     how many times to print JUNK (default=1)\n"
msg += "  -h, --help     shows this help message and exits\n"
msg += "  -v, --version     shows the version number and exits\n"

data = docopt(msg, version="1.0")
n = int(data["--count"])

for i in range(n):
    print("JUNK")

   I didn't test it, but that should work or be really close. How you use parameters is up to you. It's
   not going to write your code for you. Just because you document something, doesn't mean it's actually
   going to get used. You have to code that.

***
   From a packaging perspective here's my project layout:
Project/
Project/setup.py
Project/setup.cfg
Project/module/
Project/module/__init__.py
Project/module/module.py
Project/module/docs/
Project/module/i18n/
Project/module/templates/
Project/module/static/
Project/module/submodule/__init__.py
Project/module/submodule/submodule.py
Project/other_module
Project/other_module/docs
Project/other_module/__init__.py
Project/other_module/other_module.py
Project/other_module/i18n/
Project/scripts/
Project/

   The setup.py/setup.cfg will be configured to install all included modules (that would be 'module' and
   'submodule') in a single .egg container (that may or may not be zip-safe). This will allow for the
   project to be installed in virtualenv containers and copied between systems without much thought
   (it's all self-contained).

   The filesystem layout created by setup.py will end up looking something like this (just one file as
   an example to save a lot of typing):
/usr/local/lib/python2.7/dist-packages/module-1.0.0-py2.7.egg/module/submodule/submodule.py

   This layout is fairly standard and it can solve/prevent a lot of issues if you stay consistent and
   use things like the [74]pkg_resources module.

   Note that if you use pkg_resources consistently your module will be zip-safe (99% of the time) which
   means it will be very portable and (more) usable on things like embedded systems (think: OpenWRT).

   Another important thing to think about is your setup.py: Make sure you understand the setup() method
   (e.g. from setuptools import setup) and all its arguments. Especially the entry_points feature
   (notably, console_scripts). I had to go through a lot of effort re-organizing [75]Gate One to resolve
   a number of platform/container portability issues and those two features basically made it all
   possible.

Other useful tidbits:
   Learn how to use setup.cfg and stdeb.cfg so that python setup.py bdist_rpm and python setup.py
   --command-packages=stdeb.command bdist_deb will work to make .rpm and .deb packages for your module
   in an automagic fashion.

   If possible write all your code to run on Python 3 from the start. As a fallback make it work in
   Python 2.7 but don't concern yourself with 2.6 or older unless you absolutely have to. This will
   prevent a lot of headaches if and when your package becomes popular! No one will ever pester you to
   "make it work" in older Python versions if you explicitly state up front that it's a Python 3.2+
   package. Trust me.

   Sometimes it's a good idea to make the use of an external dependency optional. When doing this make
   sure you issue something like logging.warning("The foo module is not installed. This will mean
   reduced functionality") when your module is imported or your script is run. I also like to give
   explicit instructions on how to install the dependency (e.g. sudo pip install foo).

   Don't assume that the system where your package is being installed has Internet access. A lot of
   corporate intranets have no means (from the server) to run things like pip install. So if you have a
   lot of external dependencies in your package make it as easy as possible for people installing it to
   find/download/install them.

   Think about how you want your package to log things: Do you want to have your own log files or should
   you use syslog? Most of the time I'd say, "use syslog" but sometimes you need to make logs in a
   proprietary way (e.g. certain things just need to go in a database or a special kind of file). Note
   that when you use syslog you get "centralized logging" support for "free" (which is why I recommend
   syslog). Also think about the different kinds of logs your module may generate. "The more separate
   logs, the better" I say. My projects usually have at least an auth.log that's separate from
   everything else.

   Just because you don't think you're going to need internationalization support doesn't mean you
   won't. It only takes a couple minutes to create that i18n directory and put something like this at
   the beginning of every .py file:
temp_locale = locale.get(os.environ.get('LANG', 'en_US').split('.')[0])
_ = temp_locale.translate

   That makes it so you can wrap strings like this:
_("Error: %s is invalid!") % somevar
# Note the % is outside of the _()

   ...when you write the code initially and add translations later if needed. It also works for
   .format():
_("The {foo} operation completed successfully!").format(foo=foo)

   A tiny amount of extra work up front can save you a ton of work later!


---
https://stackoverflow.com/questions/193161/what-is-the-best-project-structure-for-a-python-application

What is the best project structure for a Python application? [closed]

   Imagine that you want to develop a non-trivial end-user desktop (not web) application in Python. What
   is the best way to structure the project's folder hierarchy?

   Desirable features are ease of maintenance, IDE-friendliness, suitability for source control
   branching/merging, and easy generation of install packages.

   In particular:
    1. Where do you put the source?
    2. Where do you put application startup scripts?
    3. Where do you put the IDE project cruft?
    4. Where do you put the unit/acceptance tests?
    5. Where do you put non-Python data such as config files?
    6. Where do you put non-Python sources such as C++ for pyd/so binary extension modules?

***
   Doesn't too much matter. Whatever makes you happy will work. There aren't a lot of silly rules
   because Python projects can be simple.
     * /scripts or /bin for that kind of command-line interface stuff
     * /tests for your tests
     * /lib for your C-language libraries
     * /doc for most documentation
     * /apidoc for the Epydoc-generated API docs.

   And the top-level directory can contain README's, Config's and whatnot.

   The hard choice is whether or not to use a /src tree. Python doesn't have a distinction between /src,
   /lib, and /bin like Java or C has.

   Since a top-level /src directory is seen by some as meaningless, your top-level directory can be the
   top-level architecture of your application.
     * /foo
     * /bar
     * /baz

   I recommend putting all of this under the "name-of-my-product" directory. So, if you're writing an
   application named quux, the directory that contains all this stuff is named /quux.

   Another project's PYTHONPATH, then, can include /path/to/quux/foo to reuse the QUUX.foo module.

***
   According to Jean-Paul Calderone's Filesystem structure of a Python project:
Project/
|-- bin/
|   |-- project
|
|-- project/
|   |-- test/
|   |   |-- __init__.py
|   |   |-- test_main.py
|   |
|   |-- __init__.py
|   |-- main.py
|
|-- setup.py
|-- README

***
Filesystem structure of a Python project

     Do:
     * name the directory something related to your project. For example, if your project is named
       "Twisted", name the top-level directory for its source files Twisted. When you do releases, you
       should include a version number suffix: Twisted-2.5.
     * create a directory Twisted/bin and put your executables there, if you have any. Don't give them a
       .py extension, even if they are Python source files. Don't put any code in them except an import
       of and call to a main function defined somewhere else in your projects. (Slight wrinkle: since on
       Windows, the interpreter is selected by the file extension, your Windows users actually do want
       the .py extension. So, when you package for Windows, you may want to add it. Unfortunately
       there's no easy distutils trick that I know of to automate this process. Considering that on
       POSIX the .py extension is a only a wart, whereas on Windows the lack is an actual bug, if your
       userbase includes Windows users, you may want to opt to just have the .py extension everywhere.)
     * If your project is expressable as a single Python source file, then put it into the directory and
       name it something related to your project. For example, Twisted/twisted.py. If you need multiple
       source files, create a package instead (Twisted/twisted/, with an empty
       Twisted/twisted/__init__.py) and place your source files in it. For example,
       Twisted/twisted/internet.py.
     * put your unit tests in a sub-package of your package (note - this means that the single Python
       source file option above was a trick - you always need at least one other file for your unit
       tests). For example, Twisted/twisted/test/. Of course, make it a package with
       Twisted/twisted/test/__init__.py. Place tests in files like
       Twisted/twisted/test/test_internet.py.
     * add Twisted/README and Twisted/setup.py to explain and install your software, respectively, if
       you're feeling nice.

     Don't:
     * put your source in a directory called src or lib. This makes it hard to run without installing.
     * put your tests outside of your Python package. This makes it hard to run the tests against an
       installed version.
     * create a package that only has a __init__.py and then put all your code into __init__.py. Just
       make a module instead of a package, it's simpler.
     * try to come up with magical hacks to make Python able to import your module or package without
       having the user add the directory containing it to their import path (either via PYTHONPATH or
       some other mechanism). You will not correctly handle all cases and users will get angry at you
       when your software doesn't work in their environment.

***
   Let me excerpt the project layout part of that excellent article:

     When setting up a project, the layout (or directory structure) is important to get right. A
     sensible layout means that potential contributors don't have to spend forever hunting for a piece
     of code; file locations are intuitive. Since we're dealing with an existing project, it means
     you'll probably need to move some stuff around.

     Let's start at the top. Most projects have a number of top-level files (like setup.py, README.md,
     requirements.txt, etc). There are then three directories that every project should have:
     * A docs directory containing project documentation
     * A directory named with the project's name which stores the actual Python package
     * A test directory in one of two places
          + Under the package directory containing test code and resources
          + As a stand-alone top level directory To get a better sense of how your files should be
            organized, here's a simplified snapshot of the layout for one of my projects, sandman:

$ pwd
~/code/sandman
$ tree
.
|- LICENSE
|- README.md
|- TODO.md
|- docs
|   |-- conf.py
|   |-- generated
|   |-- index.rst
|   |-- installation.rst
|   |-- modules.rst
|   |-- quickstart.rst
|   |-- sandman.rst
|- requirements.txt
|- sandman
|   |-- __init__.py
|   |-- exception.py
|   |-- model.py
|   |-- sandman.py
|   |-- test
|       |-- models.py
|       |-- test_sandman.py
|- setup.py

     As you can see, there are some top level files, a docs directory (generated is an empty directory
     where sphinx will put the generated documentation), a sandman directory, and a test directory
     under sandman.

***
   Try starting the project using the python_boilerplate template. It largely follows the best
   practices (e.g. [94]those here), but is better suited in case you find yourself willing to split your
   project into more than one egg at some point (and believe me, with anything but the simplest
   projects, you will. One common situation is where you have to use a locally-modified version of
   someone else's library).
     * Where do you put the source?
          + For decently large projects it makes sense to split the source into several eggs. Each egg
            would go as a separate setuptools-layout under PROJECT_ROOT/src/<egg_name>.
     * Where do you put application startup scripts?
          + The ideal option is to have application startup script registered as an entry_point in one
            of the eggs.
     * Where do you put the IDE project cruft?
          + Depends on the IDE. Many of them keep their stuff in PROJECT_ROOT/.<something> in the root
            of the project, and this is fine.
     * Where do you put the unit/acceptance tests?
          + Each egg has a separate set of tests, kept in its PROJECT_ROOT/src/<egg_name>/tests
            directory. I personally prefer to use py.test to run them.
     * Where do you put non-Python data such as config files?
          + It depends. There can be different types of non-Python data.
               o "Resources", i.e. data that must be packaged within an egg. This data goes into the
                 corresponding egg directory, somewhere within package namespace. It can be used via
                 pkg_resources package.
               o "Config-files", i.e. non-Python files that are to be regarded as external to the
                 project source files, but have to be initialized with some values when application
                 starts running. During development I prefer to keep such files in PROJECT_ROOT/config.
                 For deployment there can be various options. On Windows one can use
                 %APP_DATA%/<app-name>/config, on Linux, /etc/<app-name> or /opt/<app-name>/config.
               o Generated files, i.e. files that may be created or modified by the application during
                 execution. I would prefer to keep them in PROJECT_ROOT/var during development, and
                 under /var during Linux deployment.
     * Where do you put non-Python sources such as C++ for pyd/so binary extension modules?
          + Into PROJECT_ROOT/src/<egg_name>/native

   Documentation would typically go into PROJECT_ROOT/doc or PROJECT_ROOT/src/<egg_name>/doc (this
   depends on whether you regard some of the eggs to be a separate large projects). Some additional
   configuration will be in files like PROJECT_ROOT/buildout.cfg and PROJECT_ROOT/setup.cfg.


---
